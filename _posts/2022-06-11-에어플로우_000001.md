---
layout: single
title:  "에어플로우1: Docker를 이용한 airflow 로컬 개발환경 구축"
author_profile: true
categories:
  - 데이터엔지니어
tags:
  - Python
  - airflow
  - docker
comments: true
---

## - 목적
이번에 회사에서 airflow를 이용한 Mlops를 목표로 하게되면서 로컬에 airflow 개발환경을 구축할 필요가 있었다. 구글링으로 찾아보니 윈도우로 airflow 환경 구축하는 것이 꽤 귀찮은 과정이 필요했다. 따라서 이번 기회에 도커까지 이용해서 개발환경을 구축해보자! 라는 마음으로 다양한 테스트를 해보았다! 버전은 2.2.3으로 진행!

[https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml](https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml)

우선 이 사이트로 들어가면 airflow에서 기본적으로 제공하는 yaml파일을 만들 수 있다.

기본 구성은 airflow, postgres, redis, webserver, scheduler, worker, triggerer, init, clit, flower 순으로 되어있다. 세부적으로 어떤 역할을 하는지는 아직 자세히는 모르기 때문에 나주에 이야기하도록 하자,

## - mysql로 변환 작업

보통 airflow를 빈깡통에 설치를 하면 SQLite로 설치가 된다. 그러나 docker-compose 내용 상에는 postgreSQL이 기본으로 되어 있었다. 그런데 나는 MySQL에 더욱 익숙하기 때문에 postgreSQL에서 MySQL로 변환하고 싶었다. 따라서 다음과 같은 작업을 해주었는데.

![postgres](/assets/images/postgres.jpg)

이 부분을 다음과 같이 변경해 주었다.

![mysql_ariflow](/assets/images/mysql_ariflow.jpg)

여기서 cap_add 부분을 넣어줘야 나중에 도커에 올렸을 때 에러가나지 않는다. 물론 추가로 airflow-common에 다음과 같이 수정해 주어야 할 것이다.

![mysql_airflow_common](/assets/images/mysql_airflow_common.jpg)

volumes에 mysql-data를 만든 이유는 도커에 올리고 도커를 내렸을 때, 사용했던 DB를 계속해서 사용하기 위해 남긴 것이다. 서비스 내에 위와 같이 남겨주고 마지막에도 다음과 같이 남겨주어야 한다.

![mysql_volumes](/assets/images/mysql_volumes.jpg)

## - 쥬피터 연동

이 부분이 쪼금 애먹었던 부분이다. Group Command 오류가 계속 떠서 다양하게 시도해보고 다음과 같은 서비스로 작성하게 되었다.

![jupyter_airflow](/assets/images/jupyter_airflow.jpg)

비밀번호는 1234를 암호화하여 넣어 놓은 것이니 다른 것으로 변경해도 된다. 이렇게 하면 대강 쥬피터에서 실행해보고 dags를 올리는 것이 가능해 진다.

## - Dockerfile

![airflow_docker](/assets/images/airflow_docker.jpg)

도커파일 구성은 다음과 같이 간단하게 진행했다. 아 참고로 requirements에 jupyterlab이 있어야 쥬피터 사용이 가능하다!

## - 마치며
이렇게 airflow 로컬 개발환경 구성이 끝났다. 물론 더 추가할 만한 컨테이너가 있으면 더 추가할 계획이고 requirement에 추가하여 진행도 가능할 것이다. 다음 포스팅은 파이썬 오퍼레이터를 통한 간단한 dag 구성을 목표로 공부해볼 계획이다. 
